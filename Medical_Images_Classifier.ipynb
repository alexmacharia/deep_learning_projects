{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLabS1XbU63OSCuRKKzO9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmacharia/deep_learning_projects/blob/main/Medical_Images_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3CkW67TGkDN",
        "outputId": "c651a829-0646-46b7-a08d-c4ba2739d471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "q7oXmABFGudo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/gdrive/MyDrive/omdena_project/medical_images/train\"\n",
        "test_path = \"/content/gdrive/MyDrive/omdena_project/medical_images/test\"\n",
        "val_path = \"/content/gdrive/MyDrive/omdena_project/medical_images/val\"\n"
      ],
      "metadata": {
        "id": "m3dcXH_nHbUE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
        "    .flow_from_directory(directory=train_path, target_size=(224,224),batch_size=10)\n",
        "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
        "    .flow_from_directory(directory=val_path, target_size=(224,224),batch_size=10)\n",
        "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
        "    .flow_from_directory(directory=test_path, target_size=(224,224),batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBLy-4BsNZl0",
        "outputId": "b977bd15-81f1-406a-f538-f33a47ef71c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6326 images belonging to 4 classes.\n",
            "Found 38 images belonging to 4 classes.\n",
            "Found 771 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnD_Lmr4NpZD",
        "outputId": "4b138042-0c18-4e32-ceb1-018303ddc04c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.preprocessing.image.DirectoryIterator at 0x7fbba67a79d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(224,224,3)),\n",
        "    MaxPool2D(pool_size=(2,2), strides=2),\n",
        "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'),\n",
        "    Flatten(),\n",
        "    Dense(units=4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "7iADiu1XObNY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
      ],
      "metadata": {
        "id": "AsiuzrWzRMH-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV_0xHL5Rfoq",
        "outputId": "8a5ff530-c7d5-4a53-f6ba-63d6e8953189"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 112, 112, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 802816)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 3211268   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,230,660\n",
            "Trainable params: 3,230,660\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=train_batches,\n",
        "                    steps_per_epoch=len(train_batches),\n",
        "                    validation_data=valid_batches,\n",
        "                    validation_steps=len(valid_batches),\n",
        "                    epochs=10,\n",
        "                    verbose=2\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuPgByhXRyO6",
        "outputId": "3405935d-e51a-46ec-e743-26d4ce7c66de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "633/633 - 3510s - loss: 19.8735 - accuracy: 0.8505 - val_loss: 1.0919 - val_accuracy: 0.6842 - 3510s/epoch - 6s/step\n",
            "Epoch 2/10\n",
            "633/633 - 526s - loss: 0.1526 - accuracy: 0.9524 - val_loss: 1.3904 - val_accuracy: 0.6842 - 526s/epoch - 831ms/step\n",
            "Epoch 3/10\n",
            "633/633 - 516s - loss: 0.0487 - accuracy: 0.9861 - val_loss: 2.2379 - val_accuracy: 0.6316 - 516s/epoch - 815ms/step\n",
            "Epoch 4/10\n",
            "633/633 - 515s - loss: 0.1579 - accuracy: 0.9687 - val_loss: 1.8335 - val_accuracy: 0.6579 - 515s/epoch - 814ms/step\n",
            "Epoch 5/10\n",
            "633/633 - 516s - loss: 0.1439 - accuracy: 0.9685 - val_loss: 2.0181 - val_accuracy: 0.6053 - 516s/epoch - 815ms/step\n",
            "Epoch 6/10\n",
            "633/633 - 519s - loss: 0.0474 - accuracy: 0.9866 - val_loss: 1.5632 - val_accuracy: 0.6579 - 519s/epoch - 820ms/step\n",
            "Epoch 7/10\n",
            "633/633 - 517s - loss: 0.0908 - accuracy: 0.9813 - val_loss: 1.3184 - val_accuracy: 0.7368 - 517s/epoch - 817ms/step\n",
            "Epoch 8/10\n",
            "633/633 - 511s - loss: 0.2544 - accuracy: 0.9682 - val_loss: 4.9157 - val_accuracy: 0.5789 - 511s/epoch - 807ms/step\n",
            "Epoch 9/10\n",
            "633/633 - 514s - loss: 0.0692 - accuracy: 0.9866 - val_loss: 2.8196 - val_accuracy: 0.6579 - 514s/epoch - 812ms/step\n",
            "Epoch 10/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VvdaVHm5SOwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}